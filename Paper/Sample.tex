\documentclass{CUP-JNL-DTM}%


%%%% Packages
\usepackage{graphicx}
\usepackage{multicol,multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{rotating}
\usepackage{appendix}
% For JDM please remove the natbib package:
\usepackage[numbers]{natbib}
% And use biblatex-apa with a .bib file to format your references according to the APA7 style.
% \usepackage[natbib,style=apa]{biblatex}
% \addbibresource{your-refs.bib}
\usepackage{ifpdf}
\usepackage[T1]{fontenc}
\usepackage{newtxtext}
\usepackage{newtxmath}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage[colorlinks,allcolors=blue]{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\numberwithin{equation}{section}


\jname{Fluids/Data}
\articletype{Research article}
%\artid{20}
\jyear{2024}
%\jvol{4}
%\jissue{1}
%\raggedbottom

\begin{document}

\begin{Frontmatter}

\title[Article Title]{Towards Higher Accuracy Diffusion Models for Predictions of Airfoil Flow Under Varying Flow Regimes}

% There is no need to include ORCID IDs in your .pdf; this information is captured by the submission portal when a manuscript is submitted. 
\author[1]{Kenechukwu Ogbuagu}

\authormark{Kenechukwu Ogbuagu }

\address[1]{\orgdiv{Department of Mechanical Engineering}, \orgname{University of Lincoln}, \orgaddress{\city{Lincoln},  \state{Lincolnshire},  \country{United Kingdom}}}

\keywords{Diffusion Model, Airfoil Prediction, Conditioning}

\keywords[MSC Codes]{\codes[Primary]{}; \codes[Secondary]{, }}

\abstract{Abstracts should be 250 words. It must be able to stand alone and so cannot contain citations to the paper's references, equations, etc. An abstract must consist of a single paragraph and be concise. Because of online formatting, abstracts must appear as plain as possible.}

\end{Frontmatter}

\section*{Impact Statement}
Some Data journals (DAP, DCE) require an `Impact Statement' section. Comment out this section if it is not required.

% Some math journals (FLO) require a table of contents. Comment out this line if no ToC is needed.
%\localtableofcontents

\section[Introduction]{Introduction}
Airfoil design is a critical process in designing aircraft and rotodynamic machinery, often relying on intricate numerical simulations and Computational Fluid Dynamics (CFD) to assess the performance and flow characteristics of candidate airfoils. These simulations are both time-consuming and expensive, which restricts their effectiveness in the vital process of acquiring the flow field around the airfoil for design and optimization purposes.[1][2] Considering the computation expense in the high-fidelity CFD simulation, surrogate models have emerged as a viable alternative, providing faster computational times and a balanced trade-off between precision and computational cost.[3]. 
There is a clear potential for deep learning to revolutionize fluid mechanics in the prediction of flow patterns as cutting-edge deep learning methods and architectures have been devised to infer Reynolds-averaged Navier–Stokes (RANS) solutions over various flow conditions[1][4].

Recently, denoising diffusion probabilistic models (DDPMs), a cutting-edge class of generative models known for outperforming previous approaches in generating highly impressive results across various related contexts, have been applied in a limited number of studies as surrogate models for predicting airfoil flows across different flow regimes. These studies aim to enhance the accuracy and efficiency of aerodynamic predictions


\subsection{The Challenge}
There are numerous modifications to the structure and implementation of denoising diffusion probabilistic models (DDPMs) that hold significant potential for improving the accuracy of their results. These modifications can enhance the model's ability to capture complex patterns in data. These suggest a promising pathway for leveraging DDPMs in more complex and high-stakes applications, including the prediction of aerodynamic flows and other challenging engineering problems. Their findings indicate a limitation that the model does not show superiority in extrapolation cases where the range of Reynolds number (Re) and angle of attack (alpha) falls outside the training dataset. Despite this, DDPM demonstrates strong generalization capabilities for new airfoil shapes, as the test dataset includes both interpolation and extrapolation scenarios involving previously unseen airfoil geometries.

\subsection{Our Contribution}
In this paper, we aim to enhance the accuracy of denoising diffusion probabilistic models (DDPMs) for the task of airfoil flow field prediction by implementing various already existing conditioning mechanisms. We intend to guide the diffusion process more effectively, ensuring that the generated samples are more precise and closely aligned with CFD results. Potentially reducing uncertainty and improving the model's ability to capture intricate patterns in aerodynamic flows. This approach is expected to lead to more precise and reliable predictions, thereby advancing the application of DDPMs in aerodynamic modeling and beyond.
\section{Airfoil Flow prediction}
In recent years various works have leveraged deep learning methodologies in various flow field predictions. Duru et al. [6] present a method that leverages deep learning to predict transonic flow fields,  The methodology implores the use convolutional encoder-decoder neural network model (CNN-FOIL). The encoder-decoder structure, which allows for shared feature extraction across tasks like Mach number and pressure field prediction. Li-Wei et al[4] propose an alternative approach to applying the spatial inputs using curvilinear coordinate transformations and structured body-fitted grids to preserve geometric information in a CNN-based model providing faster and more accurate predictions of flow fields.

In 2017 Vaswani et al [7] proposed a new simple network architecture, the Transformer, based solely on attention mechanisms. Attention mechanisms work by computing a set of attention scores that determine the importance of each part of the input in relation to others. The attention mechanism is designed to enable the model to focus on relevant parts of the input data by assigning different weights to different elements of the input sequence. This allows the Transformer to capture long-range dependencies more efficiently than traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs). This attention mechanism has been applied to fluid flow prediction in various forms. Wang et al [2], employ a self-attention module to capture diverse information within and between flow fields. Their CNN leverages this information, focusing on stronger relationships irrespective of distances, to enhance flow field prediction results. Quantitative results show that their proposed model achieves higher accuracy in predicting both original and optimized airfoil flow fields. 

Zuo et al. [8] introduce an innovative deep-learning framework designed for the swift reconstruction of airfoil flow fields. By incorporating channel attention and spatial attention modules during the down-sampling phases of a unet, they significantly improve the feature learning ability of the model[9]. This advancement notably reduces training loss and boosts the accuracy of flow field predictions, making the prediction efficiency of their neural network model vastly superior to traditional CFD computational methods.

****************************************
Liu et al[9] provide one of the first uses of denoising diffusion probabilistic models (DDPMs) in airfoil flow predition  They assert that this approach allows for generating high resolution samples from the distribution of solutions. Their findings indicate a limitation that the model does not show superiority in extrapolation cases where the range of Reynolds number (Re) and angle of attack (alpha) falls outside the training dataset. Despite this, DDPM demonstrates strong generalization capabilities for new airfoil shapes, as the test dataset includes both interpolation and extrapolation scenarios involving previously unseen airfoil geometries. This performance may be attributed to the convolutional UNet's effectiveness in capturing spatial hierarchies, rather than changes in magnitude.

\subsection{Diffusion Models}

Diffusion models generate samples from a distribution by reversing a gradual noising process. Specifically, sampling begins with an initial noisy input and progressively produces less noisy samples and so on, until reaching the final sample $x^T $.
Each timestep $(t)$ corresponds to a specific noise level, and $x(t)$ can be viewed as a combination of the signal $x(0)$ and some noise, with the signal-to-noise ratio determined by the timestep $(t)$. 

\[
p_{\theta}(x_{i}^{0:T}) =p(x_{i}^{T}) \prod_{t=1}^{T} p(x_{i}^{t-1}|x_{i}^{t})
\]
A diffusion model is trained to produce a slightly more "denoised" version $x(t−1)$ from $x(t)$.  Ho et al.[10]  describe this model as a function x(t)and t that predicts the noise component within a noisy sample x(t) To train these models, a minibatch is created by randomly selecting a data sample $x(0)$, a timestep t, and noise ϵ, which together form a noisy sample x(t) The training objective involves minimizing the mean-squared error loss[11] between the actual noise and the predicted noise. 
\[
 L_{NN}(\theta)=E_{x,\epsilon,\sim N(0,I)}[||\epsilon - \epsilon_{\theta}(x_{i}^t,t)||^2]
\]
Following the conditioning approach of Lyu et al. [87–90], the physical parameters are added to the input as condition c in $\epsilon_{\theta}(x_{i}^t,c,t)$ in other to synthesize $(x_{i}^{0:T})$ resulting in the loss function 
\[
 L_{NN}(\theta)=E_{x,\epsilon,\sim N(0,I)}[||\epsilon - \epsilon_{\theta}(x_{i}^t,c,t)||^2]
\] 
The combination of conditioning and guidance mechanisms in DPPMs has played a pivotal role in enhancing the performance of diffusion models. Different conditions, such as labels, classifiers, texts, images, semantic maps, graphs, and so on, are widely used to guide the generation directions of diffusion models. 

There are mainly four kinds of conditioning mechanisms, including concatenation, gradient-based, cross-attention and adaptive layer normalization (adaLN). The concatenation means diffusion models concatenate informative guidance with intermediate denoised targets in diffusion process, such as label embedding and semantic feature maps. The gradient-based mechanism incorporates task-related gradient into the diffusion sampling process for controllable generation. For example, in image generation, one can train an auxiliary classifier on noisy images, and then use gradients to guide the diffusion sampling process towards an arbitrary class label. The cross-attention performs attentional message passing between the guidance and diffusion targets, which is usually conducted in a layer-wise manner in denoising networks. The adaLN mechanism follows the widespread usage of adaptive normalization layers (Perez et al., 2018) in GANs (Karras et al., 2019), Scalable Diffusion Models (Peebles and Xie, 2022) explores replacing standard layer norm layers in transformer-based diffusion backbones with adaptive layer normalization. Instead of directly learning dimension-wise scale and shift parameters, it regresses them from the sum of the time embedding and conditions.These innovations not only improved the quality of generated outputs but have also expanded the applicability of diffusion models to complex, condition-dependent tasks such as airfoil flow prediction. Our research centres around further improvements in the accuracy of DDPMs for this task. 

\subsection{Unet Architecture}

The U-Net architecture consists of an encoder and a decoder, connected by skip connections that transfer feature maps from the encoder to the corresponding layers in the decoder. This structure helps the model retain high-resolution details lost during down-sampling in the encoder. The encoder progressively reduces the spatial dimensions of the input while increasing the depth of feature maps, effectively capturing global context. The decoder then gradually reconstructs the data back to its original resolution, integrating the information from the skip connections to recover fine details.


\footnote{This work is derivative of ....}

\section{Methodology}
This chapter details the methodology used to train a Diffusion Denoising Probabilistic Model (DPM). Our approach draws inspiration from methods traditionally used for text-based conditioning in DDPMs but adapts these techniques to better capture key aerodynamic features such as Reynolds number $(Re)$ and angle of attack $(\alpha)$. The model’s performance is compared to a baseline model implemented by Thürey et al., providing insights into the benefits of our proposed approach.

\subsection{Model Architecture}
To better capture the influence of the physical parameters $(Re and \alpha)$, we incorporate cross-attention mechanisms within the U-Net. Cross-attention allows the model to dynamically attend to different aspects of the input condition at each layer of the network. This is particularly useful in scenarios where the condition has a complex relationship with the output.

\subsubsection{Conditioning with Concatenation and Cross Attention}
The physical parameters are injected into the U-Net using cross-attention, while spatial information is still integrated by concatenating the input with the condition. With the addition of cross attention to each layer the number of learnable parameters and the computation time increases 


\subsubsection{Conditioning with only Cross Attention}



\subsection{Training Procedure}
The model is trained on a dataset consisting of flow field simulations around airfoils at various Reynolds numbers and angles of attack. Each sample in the dataset includes the flow field (as a grid of velocity vectors) and the corresponding physical parameters $(Re, \alpha)$.

Before training, the flow field data is normalized to have zero mean and unit variance, ensuring that all features are on the same scale, which is crucial for stable training. The Reynolds number and angle of attack are also normalized based on the range of values in the dataset. These normalized physical parameters are then embedded into a higher-dimensional space using a series of fully connected layers.
he model is trained using the Adam optimizer, with a learning rate of. The training process runs for 500,000 iterations, with mini-batches of 64 samples. The model's performance is evaluated at regular intervals using a validation set, and the best model is selected based on the lowest validation loss.
\subsection{Comparison with Thürey et al}
To assess the effectiveness of our approach, we compare the model to that implemented by Thürey et al. Their approach involves conditioning the diffusion model using a simpler concatenation method, without the use of cross-attention. While effective, this method does not allow for the dynamic focusing capability provided by cross-attention, potentially limiting the model’s ability to capture complex dependencies between the physical parameters and the flow field.

\subsection{Implementation of the Baseline Model}


\section{Results}





Tables can be inserted via the normal table and tabular environment. To put
footnotes inside tables one has to use the additional ``fntable" environment
enclosing the tabular environment. The footnote appears just below the table
itself.

\begin{table}[t]
\tabcolsep=0pt%
\TBL{\caption{Model Parameters and Performance\label{tab2}}}
{\begin{fntable}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccccc@{}}\toprule%
 & \multicolumn{3}{@{}c@{}}{\TCH{Concatenation}}& \multicolumn{3}{@{}c@{}}{\TCH{No Concatenation\smash{\footnotemark[1]}}}
 \\\cmidrule{2-4}\cmidrule{5-7}%
\TCH{Model} & \TCH{No of Parameters} & \TCH{$\sigma_{\mathit{calc}}$} & \TCH{$\sigma_{\mathit{expt}}$} &
\TCH{Energy} & \TCH{$\sigma_{\mathit{calc}}$} & \TCH{$\sigma_{\mathit{expt}}$} \\\midrule

{\TCH{Cross attention in zero(0) layers}}&500 A &961 &$\hphantom{0}922\pm10$ &900 A &1268 &$1092\pm40\hphantom{0}$\\

\TCH{Cross attention in Two(2) layers}&990 A &1168 &$1547\pm12$ &780 A &1166 &$1239\pm100$\\

{\TCH{Cross attention in four(4) layers}}&500 A &961 &$\hphantom{0}922\pm10$ &900 A &1268 &$1092\pm40\hphantom{0}$\\
\botrule
\end{tabular*}%
\footnotetext[]{{Note:} *}
\footnotetext[1]{**}%
\end{fntable}}
\end{table}

\section{Conclusion}
This study focused on enhancing Diffusion Probabilistic Models (DPMs) for predicting flow field dynamics around airfoils to improve performance in extrapolation cases where the range of Reynolds number $(Re)$ and angle of attack $(\alpha)$ fall outside the training dataset by incorporating these parameters, into the U-Net architecture using cross-attention mechanisms. By dynamically focusing on these parameters, the model demonstrated superior accuracy and physical consistency in predicting flow fields compared to a baseline model of Thürey et al.
includes both interpolation and extrapolation scenarios involving previously unseen airfoil geometries.
The use of cross-attention allowed the model to better capture the complex relationships between $(Re, \alpha)$ and the generated outputs, while also maintaining spatial information through input concatenation. Despite challenges in balancing the attention mechanism within the architecture, the methodology proved effective in improving DDPM performance for aerodynamic simulations.

The findings have significant implications for computational fluid dynamics, showing that cross-attention can enhance generative models for condition-specific tasks. However, limitations related to training data uncertainties, diversity and increased computational cost suggest areas for further research. Future work could explore other conditioning and guidance mechanisms including but not limited to attention masking and adversarial blurring, and adaptive layer normalization to extend this approach to different domains, or optimize the model for broader applications.

In summary, this study successfully advanced the use of DDPMs in fluid dynamics, providing a foundation for future improvements in generative modelling for complex, condition-dependent scenarios.


\begin{Backmatter}

\paragraph{Acknowledgments}
We are grateful for the discussions and feedbacks from Manuel Herrera, Marco Perez Hernandez, and Amit Kumar Jain from the Institute for Manufacturing, Cambridge CB3 0FS, UK


\paragraph{Funding Statement}
This research was funded by the EPSRC and BT Prosperity Partnership project: Next Generation Converged Digital Infrastructure, grant number EP/R004935/1. This research was also funded by Siemens Industrial Turbomachinery, Lincoln, UK LN5 7FD.

\paragraph{Competing Interests}
A statement about any financial, professional, contractual or personal relationships or situations that could be perceived to impact the presentation of the work --- or `None' if none exist

\paragraph{Data Availability Statement}
The data used to generate the results presented in this paper can be found in: https://github.com/Dhada27/Hierarchical-Modelling-Asset-Fleets

\paragraph{Ethical Standards}
The research meets all ethical guidelines, including adherence to the legal requirements of the study country.

\paragraph{Author Contributions}
{\verb+\url{https://www.casrai.org/credit.html}+}. Conceptualization: A.A; A.B. Methodology: A.A; A.B. Data curation: A.C. Data visualisation: A.C. Writing original draft: A.A; A.B. All authors approved the final submitted draft.


\begin{thebibliography}{}
\bibitem[Ananin A. and Mironov A.(2000)]{bib1}
\textbf{Ananin A. and Mironov A.} (2000) The moduli space of $2$-dimensional algebras, \textit{Comm. Algebra} \textit{28}(9),  {4481}--{4488}.

\bibitem[Bai C. and Meng D.(2001)]{bib2}
\textbf{Bai C. and Meng D.} (2001) The classification of Novikov algebras in low dimension,  \textit{J. Phys. A: Math. Gen.} \textit{34}, {1581}--{1594}.

\bibitem[Ca\~{n}ete E. and Khudoyberdiyev A.(2013)]{bib3}
\textbf{Ca\~{n}ete E. and Khudoyberdiyev A.} (2013) The classification of $4$-dimensional Leibniz algebras,  \textit{Linear Algebra and its Applications}  \textit{439}(1), {273}--{288}.

\bibitem[Goze M. and Remm E.(2011)]{bib4}
\textbf{Goze M. and Remm E.} (2011)  $2$-dimensional algebras,  \textit{Afr. J. Math. Phys.} \textit{10}(1),  {81}--{91}.

\bibitem[Petersson H.(2000)]{bib5}
\textbf{Petersson H.} (2000) The classification of two-dimensional nonassociative algebras,  \textit{Results Math} \textit{37}, no. 1-2,  {120}--{154}.


\end{thebibliography}

\end{Backmatter}

\end{document}
