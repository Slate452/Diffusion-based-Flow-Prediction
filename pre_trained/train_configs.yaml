device: cuda:0
batch_size_train: 25
# we use batchsize 25 is just because we have 125 samples in the training set. You can increase it if you have more samples. Like 50 in our manuscript when training with larger dataset
shuffle_train: true
num_workers_train: 0
validation_epoch_frequency: 0
optimizer: AdamW
lr_scheduler: step
warmup_epoch: 0
record_iteration_loss: False
epochs: 125000
save_epoch: 5000
lr: 0.0001
final_lr: 0.00001